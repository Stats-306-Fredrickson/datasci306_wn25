---
subtitle: "Stats 306: Lecture 22"
title: "Parallel Programming"
author: "Mark Fredrickson"
output: 
  learnr::tutorial:
    progressive: true
    css: css/lecture.css
runtime: shiny_prerendered
---


```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
library(lubridate)
library(foreach)
library(doParallel)
```


## Parallel programming

Our solutions thus far have targeted **serial processing**: doing one operation at a time, in a specific order.

With more than one processor (core), how can we make use of increased **parallel** computing power?

Strategy: break up overall work into non-interacting components, send the work to different compute units, reassemble at the end.

## Parallel programming in R

There are several related packages for performing parallel computations in R:

* `parallel`: In the base set of packages, provides basic interface
* `foreach`: Not a parallel package, but another way of performing `for` loops and mapping
* `doParallel`: The parallel back-end for `foreach`

## Clusters

Most of R's parallelism is (mostly) organized around **clusters** -- collections of R processes.

* Our main R program is the *controller*
* We send work to *workers*

When we start up the collection of workers, we often have to initialize them with the libraries, code, and data we need them to use.

## When is parallelization faster?

**Amdahl's Law** states that the overall improvement in efficiency $S$ will be related to the proportion of the program that can be parallized $p$ and the number of processors used $k$ in the following way:

$$S = \frac{1}{1 - p + p/k}$$

Example: suppose 50% of the program can be parallelized and we have 4 processors:

```{r}
1 / (1 - 0.5 + 0.5 / 4)
```

If we could increase the share that could be parallelized to 75%:
```{r}
1 / (1 - 0.75 + 0.75 / 4)
```

Staying at 50%, we double the number of processors:

```{r}
1 / (1 - 0.5 + 0.5 / 8)
```

Generally, we get bigger payoffs from improving the proportion that can be done in parallel, moreso than throwing more hardware at the problem (but it doesn't hurt!).


## Exercise

Consider this method of finding the minimum in a vector:

```{r}

find_min <- function(x) {
  smallest <- Inf
  for (i in x) {
    if (i < smallest)
      smallest <- i
  }
  return(smallest)
}
find_min(c(8, 1, 9, 10, 4, 2))
```

What proportion can be done in parallel?

Is there a way that we could make a larger portion of the program run in parallel?


## Practical increases

We benefit when large portions of the program can be parallelized and the number of processors is large.

Amdahl's Law does not include the **overhead** of starting up clusters, breaking the work into pieces, communication, and reassembling the results after parallel computation. This can be significant.

## Starting with the `parallel` library

How much work can we split up on this computer?
```{r}
library(parallel)
detectCores()
```

Two ways to use these cores:

* Start **workers** and send them tasks directly
* Use a higher level iteration technique

## Exercise

In the console, find out how many cores you have in your R process.

## Starting a cluster

```{r}
cl <- makeCluster(2) # two workers on the local machine
clusterCall(cl, print, "hello")
clusterEvalQ(cl, Sys.getpid())
water <- read_csv("data/BKB_WaterQualityData_2020084.csv")
```
```{r eval = FALSE}
# this would cause an error: clusterEvalQ(cl, dim(water))
clusterEvalQ(cl, library(tidyverse))
clusterExport(cl, "water")
clusterEvalQ(cl, dim(water))
```
```{r echo = FALSE}
## This is just for the Rmarkdown which otherwise can't find "water"
clusterEvalQ(cl, library(tidyverse))
clusterExport(cl, "water", envir = environment())
clusterEvalQ(cl, dim(water))
```

## Doing something more interesting

Summarize columns in the two workers:

```{r}
clusterSplit(cl, colnames(water)) 
clusterSplit(cl, colnames(water)) |>
  clusterMap(cl = cl, fun = function(names) { summary(water[, names]) })
```

## Exercise

* Create a cluster (2 workers is enough)
* User clusterEvalQ to load the tidyverse library in each
* Use `clusterSplit` to break up `mpg$cty`
* Use `clusterMap` to find the smallest value in that group (you can use `min`)
* Combine your results

## Easier interfaces

We noted with iteration, R has built in versions of `map` and `map_*` called `lapply` and `sapply`. There are parallel versions that very easy to use:

```{r}
parLapply(cl, water, summary)
parSapply(cl, water, function(col) { sum(!is.na(col)) })
```

## Putting the cluster to bed

When you are done you can stop the cluster with:

```{r}
stopCluster(cl)
```

## Default cluster

If you only ever have one cluster you can run

```{r, eval = FALSE}
makeCluster() |> setDefaultCluster()
```

Then all of the `cl` arguments will be set automatically.

To end the cluster:
```{r, eval = FALSE}
getDefaultCluster() |> stopCluster()
```

## Shared memory (Mac/Linux)

In the previous examples all of the workers were separate from the main process.

* Pro: could even have workers on other machines
* Con: need to spend time sending data around

On Mac and Linux, you also can used "shared memory" parallelism with the `mclapply` function.

```{r}
getOption("mc.cores") # default is 2 otherwise
mclapply(water, summary) # didn't need to set up a cluster
```

## Example

Here is a contrived example that "sleeps" (does nothing) to make it slow:

```{r}
slow_function <- function(x) {
  Sys.sleep(0.5) # does nothing for `1/2 second
  x^2
}

a <- now()
slow_function(999)
now() - a
```

## Trying speeding up with parallelization

```{r}
bench::mark(lapply(1:6, slow_function),
            mclapply(1:6, slow_function, mc.cores = 2),
            memory = FALSE) # turn off when using parallel tools
```

## Is parallelization always faster?

```{r}
bench::mark(lapply(1:10000, function(i) { log(i^2) }),
            mclapply(1:10000, function(i) { log(i^2) }),
            memory = FALSE)
```

In this case, the cost of setting up the workers dominates compared to the actual computations.

## Exercise

Using either `parLapply` (any, including Windows) or `mclapply` (Mac, Linux only), iterate over all the **rows** in the `starwars` data base and compute the ratio of `mass` and `height`.

```{r parex, exercise = TRUE}

```

## `foreach` and `doParallel` libraries

Not every problem fits well in the `parLapply`/`mclapply` paradigm. For example, how would would we find a minimum using these tools?

The `foreach` library presents another way to run for loops:

```{r}
foreach(i = 1:3) %do% sqrt(i)
```

```{r}
foreach(i = 1:3, j = c("a", "b", "c")) %do% {
  v <- rep(j, i)
  paste0(v, collapse = ".")
}
```

## Apply and combine

In the previous result, we got lists as values (the most generic collection type).

If we want to **combine** our results after **applying** our code we can use the `.combine` argument:

```{r}
foreach(i = 1:3, j = c("a", "b", "c"), .combine = c) %do% {
  v <- rep(j, i)
  paste0(v, collapse = ".")
}
```

## Split-apply-combine

A common technique for parallel computation is to split, then apply, then combine.

* Split: break your big problem into smaller pieces
* Apply: on each core, do a portion of the work
* Combine: bring the results back together

## Exercise

Use split-apply-combine to find the minimum in x.

```{r sac-serial, exercise = TRUE}
# split
x <- c(33, 2, 19, -5, 38, 19)
```

```{r sac-serial-solution}
x <- c(33, 2, 19, -5, 38, 19)
x_split <- list(x[1:3], x[4:6])

# apply
foreach (i = x_split) %do% {
  min(i)
}

# apply with combine
foreach (i = x_split, .combine = min) %do% {
  min(i)
}
```

## Working in parallel

After we load the `doParallel` library

```{r}
library(doParallel)
```

We can replace `%do%` with `%doPar` but first we need to register our cluster:

```{r}
registerDoParallel(cl)
```

Then we simply replace: 

```{r eval = FALSE}
# split
x <- c(33, 2, 19, -5, 38, 19)

# apply with combine
foreach (i = clusterSplit(cl, x), .combine = min) %dopar% {
  min(i)
}
```

(for some reason this wasn't getting along with the `learnr` package so we will need to run it in the console)

## Exercise

Using `clusterSplit`, `stringr::words`, and `+`, find the total number of words in `stringr::words` that have the letters "th" in them.





